{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c36a7d5",
   "metadata": {},
   "source": [
    "# Tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a1d2e",
   "metadata": {},
   "source": [
    "##### Tokenization is the process of splitting text into smaller units called tokens. Tokens can be words, characters, or subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bcd2f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ead76a",
   "metadata": {},
   "source": [
    "### WORD TOKENIZER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a797a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "'m\n",
      "Shiva\n",
      "Prasad\n",
      ",\n",
      "a\n",
      "Machine\n",
      "Learning\n",
      "and\n",
      "AI\n",
      "practitioner\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I'm Shiva Prasad, a Machine Learning and AI practitioner!\"\n",
    "words = word_tokenize(sentence)\n",
    "for i in words:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fecb93e",
   "metadata": {},
   "source": [
    "### SENTENCE TOKENIZER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e167bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "I'm Shiva Prasad.\n",
      "I love working on Machine Learning and AI projects.\n",
      "Recently, I attended a workshop at IIT Hyderabad.\n",
      "It was an amazing experience!\n"
     ]
    }
   ],
   "source": [
    "# 'Corpus' is nothing but a 'paragraph' and a 'document' is called as 'Sentence'\n",
    "Corpus = \"Hello! I'm Shiva Prasad. I love working on Machine Learning and AI projects. Recently, I attended a workshop at IIT Hyderabad. It was an amazing experience!\"\n",
    "sentences = sent_tokenize(Corpus)\n",
    "for i in sentences:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77439d16",
   "metadata": {},
   "source": [
    "### Different scenarios based:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a623414",
   "metadata": {},
   "source": [
    "#### 1. Informal style text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "53f35e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm Shiva Prasad, and I've been learning ML & AI for months.\", \"Can't wait to apply these skills!\"]\n",
      "['I', \"'m\", 'Shiva', 'Prasad', ',', 'and', 'I', \"'ve\", 'been', 'learning', 'ML', '&', 'AI', 'for', 'months', '.', 'Ca', \"n't\", 'wait', 'to', 'apply', 'these', 'skills', '!']\n"
     ]
    }
   ],
   "source": [
    "para1 = \"I'm Shiva Prasad, and I've been learning ML & AI for months. Can't wait to apply these skills!\"\n",
    "print(sent_tokenize(para1))\n",
    "print(word_tokenize(para1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c823a9e",
   "metadata": {},
   "source": [
    "#### 2. Commas & Parentheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "98efe66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My projects (surface crack detection, dataset preparation) use YOLOv8 and Flask for deployment.']\n",
      "['My', 'projects', '(', 'surface', 'crack', 'detection', ',', 'dataset', 'preparation', ')', 'use', 'YOLOv8', 'and', 'Flask', 'for', 'deployment', '.']\n"
     ]
    }
   ],
   "source": [
    "para2 = \"My projects (surface crack detection, dataset preparation) use YOLOv8 and Flask for deployment.\"\n",
    "print(sent_tokenize(para2))\n",
    "print(word_tokenize(para2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61c805e",
   "metadata": {},
   "source": [
    "#### 3. Acronyms & Technical Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "43347a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP, AI, DL are my core interests.', 'I recently completed a workshop on Generative AI at T-Hub.']\n",
      "['NLP', ',', 'AI', ',', 'DL', 'are', 'my', 'core', 'interests', '.', 'I', 'recently', 'completed', 'a', 'workshop', 'on', 'Generative', 'AI', 'at', 'T-Hub', '.']\n"
     ]
    }
   ],
   "source": [
    "para3 = \"NLP, AI, DL are my core interests. I recently completed a workshop on Generative AI at T-Hub.\"\n",
    "print(sent_tokenize(para3))\n",
    "print(word_tokenize(para3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b14fc6f",
   "metadata": {},
   "source": [
    "#### 4. Questions & Exclamations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8c6aff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How do I improve my NLP skills?', 'What projects should I tackle next?', 'Excited for new challenges!']\n",
      "['How', 'do', 'I', 'improve', 'my', 'NLP', 'skills', '?', 'What', 'projects', 'should', 'I', 'tackle', 'next', '?', 'Excited', 'for', 'new', 'challenges', '!']\n"
     ]
    }
   ],
   "source": [
    "para4 = \"How do I improve my NLP skills? What projects should I tackle next? Excited for new challenges!\"\n",
    "print(sent_tokenize(para4))\n",
    "print(word_tokenize(para4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c3ae30",
   "metadata": {},
   "source": [
    "#### 5. Multiple Sentences with Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a23c2b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"A recruiter said, 'Filter resumes based on the job description.'\", \"I'm working on building that ML model.\"]\n",
      "['A', 'recruiter', 'said', ',', \"'Filter\", 'resumes', 'based', 'on', 'the', 'job', 'description', '.', \"'\", 'I', \"'m\", 'working', 'on', 'building', 'that', 'ML', 'model', '.']\n"
     ]
    }
   ],
   "source": [
    "para5 = \"A recruiter said, 'Filter resumes based on the job description.' I'm working on building that ML model.\"\n",
    "print(sent_tokenize(para5))\n",
    "print(word_tokenize(para5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a6d815",
   "metadata": {},
   "source": [
    "#### 6. URLs & Email-like Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1d26fa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For queries, contact me at shiva.prasad@example.com or visit http://shivaprasad.ai.']\n",
      "['For', 'queries', ',', 'contact', 'me', 'at', 'shiva.prasad', '@', 'example.com', 'or', 'visit', 'http', ':', '//shivaprasad.ai', '.']\n"
     ]
    }
   ],
   "source": [
    "para6 = \"For queries, contact me at shiva.prasad@example.com or visit http://shivaprasad.ai.\"\n",
    "print(sent_tokenize(para6))\n",
    "print(word_tokenize(para6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482a92e1",
   "metadata": {},
   "source": [
    "#### 7. Numbers and Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b6d6e332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I started my ML journey in 2023.', 'By 2025, I aim to master transformers and LLMs.']\n",
      "['I', 'started', 'my', 'ML', 'journey', 'in', '2023', '.', 'By', '2025', ',', 'I', 'aim', 'to', 'master', 'transformers', 'and', 'LLMs', '.']\n"
     ]
    }
   ],
   "source": [
    "para7 = \"I started my ML journey in 2023. By 2025, I aim to master transformers and LLMs.\"\n",
    "print(sent_tokenize(para7))\n",
    "print(word_tokenize(para7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e871711e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1459da08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b878d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
